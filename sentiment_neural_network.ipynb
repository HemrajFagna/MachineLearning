{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of l2:  423\n",
      "featureSet len:  5331\n",
      "After pos\n",
      "featureSet len:  5331\n",
      "After random\n",
      "After training\n",
      "9596\n",
      "Epoch  1  completed out of  10 , loss  249044.7716064453\n",
      "Epoch  2  completed out of  10 , loss  120561.58612060547\n",
      "Epoch  3  completed out of  10 , loss  74895.1994934082\n",
      "Epoch  4  completed out of  10 , loss  47944.176193237305\n",
      "Epoch  5  completed out of  10 , loss  33127.80347442627\n",
      "Epoch  6  completed out of  10 , loss  26397.741241455078\n",
      "Epoch  7  completed out of  10 , loss  20259.56967163086\n",
      "Epoch  8  completed out of  10 , loss  16680.198356628418\n",
      "Epoch  9  completed out of  10 , loss  11918.138452529907\n",
      "Epoch  10  completed out of  10 , loss  12110.612785339355\n",
      "Accuracy:  0.55722326\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "hm_lines = 10000000\n",
    "\n",
    "\n",
    "def create_lexicon(pos,neg):\n",
    "    lexicon = []\n",
    "    for fi in [pos,neg]:\n",
    "        with open(fi,'r') as f :\n",
    "            contents = f.readlines()\n",
    "            for l in contents[:hm_lines]:\n",
    "                all_words = word_tokenize(l.lower())\n",
    "                lexicon += list(all_words)\n",
    "                \n",
    "\n",
    "    lexicon = [lemmatizer.lemmatize(i) for i in lexicon]\n",
    "    w_counts = Counter(lexicon)\n",
    "    l2 = []\n",
    "    #print(w_counts)\n",
    "    for w in w_counts:\n",
    "        if 50<w_counts[w]<1000:\n",
    "            l2.append(w)\n",
    "    \n",
    "    print(\"Lenght of l2: \",len(l2))\n",
    "    \n",
    "    return l2    \n",
    "\n",
    "\n",
    "\n",
    "def sample_handling(sample, lexicon, classification):\n",
    "    featureset = []\n",
    "    with open(sample,'r') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents[:hm_lines]:\n",
    "            current_words = word_tokenize(l)\n",
    "            current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
    "            features = np.zeros(len(lexicon))\n",
    "            for word in current_words:\n",
    "                if word.lower() in lexicon:\n",
    "                    index_value = lexicon.index(word.lower())\n",
    "                    features[index_value] += 1 \n",
    "            \n",
    "            features = list(features)\n",
    "            featureset.append([features, classification])\n",
    "        \n",
    "    print(\"featureSet len: \",len(featureset))\n",
    "    return featureset    \n",
    "\n",
    "\n",
    "\n",
    "def create_features_sets_and_labels(pos, neg, test_size = 0.1):\n",
    "    lexicon = create_lexicon(pos,neg)\n",
    "    features = []\n",
    "    features += sample_handling(pos, lexicon, [1,0])\n",
    "    print(\"After pos\")\n",
    "    features += sample_handling(neg, lexicon, [0,1])\n",
    "    \n",
    "    random.shuffle(features)\n",
    "    print(\"After random\")\n",
    "    features = np.array(features)\n",
    "    testing_size = int(test_size*len(features))\n",
    "                                                  #       features      label(neg)                label(pos)\n",
    "    train_x = list(features[:,0][:-testing_size]) # [ [[1 0 0 1 1 1 0..],[0 1]], [[1 0 1 1 0 1 0..],[1 0]] ]\n",
    "                                                #           0              1             0            1\n",
    "    train_y = list(features[:,1][:-testing_size]) # labels for neg features \n",
    "    # read above 2 exp as from list of list read 0th data in list and store it from 0 to -testing size \n",
    "    print(\"After training\")\n",
    "    test_x = list(features[:,0][-testing_size:])\n",
    "    test_y = list(features[:,1][-testing_size:])\n",
    "    \n",
    "    print(len(train_y))\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "\n",
    "train_x, train_y, test_x, test_y = create_features_sets_and_labels('pos.txt', 'neg.txt')\n",
    "\n",
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "\n",
    "'''\n",
    "In machine learning, an epoch is a full iteration over samples. Here, we are restricting the model\n",
    "to 10 complete epochs or cycles of the algorithm running through the dataset.\n",
    "\n",
    "The batch variable determines the amount of data being fed to the algorithm\n",
    "at any given time, in this case, 100 images.\n",
    "'''\n",
    "n_classes = 2\n",
    "batch_size = 100\n",
    "\n",
    "# Height x Weight matrix\n",
    "# None represents batch size\n",
    "\n",
    "'''\n",
    "The method tf.placeholder allows us to create variables that act as nodes holding the data.\n",
    "Here, x is a 2-dimensionall array holding the MNIST images, with none implying the batch size\n",
    "(which can be of any size) and 784 being a single 28×28 image. y is the target output class that\n",
    "consists of a 2-dimensional array of 10 classes (denoting the numbers 0-9) that identify what digit is stored in each image.\n",
    "\n",
    "'''\n",
    "x = tf.placeholder('float', [None, len(train_x[0])] )\n",
    "y = tf.placeholder('float') #label for x\n",
    "\n",
    "\n",
    "def neural_network_model(data):\n",
    "    hidden_layer_1 = {'weights': tf.Variable(tf.random_normal([ len(train_x[0]) ,n_nodes_hl1])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    \n",
    "    hidden_layer_2 = {'weights': tf.Variable(tf.random_normal([n_nodes_hl1,n_nodes_hl2])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    \n",
    "    hidden_layer_3 = {'weights': tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_hl3])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    \n",
    "    output_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl3,n_classes])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    l1 = tf.add(tf.matmul(data,hidden_layer_1['weights']), hidden_layer_1['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    \n",
    "    l2 = tf.add(tf.matmul(l1,hidden_layer_2['weights']), hidden_layer_2['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    \n",
    "    l3 = tf.add(tf.matmul(l2,hidden_layer_3['weights']), hidden_layer_3['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    output = tf.matmul(l3,output_layer['weights']) + output_layer['biases']\n",
    "    \n",
    "    return output\n",
    "    \n",
    "\n",
    "''' We will be using a simple softmax model to implement our network. Softmax is a generalization of logistic regression,\n",
    "usually used in the final layer of a network. It is useful because it helps in multi-classification models where a given\n",
    "output can be a list of many different things.\n",
    "It provides values between 0 to 1 that in addition give you the probability of the output belonging to a particular class. \n",
    "'''\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "\n",
    "    ''' This is the cost function of the model – a cost function is a difference between the predicted value and\n",
    "    the actual value that we are trying to minimize to improve the accuracy of the model'''\n",
    "    \n",
    "    cost  = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = prediction,labels = y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    hm_epochs = 10\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            i=0\n",
    "            while i < len(train_x):\n",
    "                start = i\n",
    "                end = i + batch_size\n",
    "                batch_x = np.array(train_x[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "                \n",
    "                _,c = sess.run([optimizer,cost], feed_dict = {x: batch_x , y: batch_y})\n",
    "                epoch_loss += c\n",
    "                i += batch_size\n",
    "                \n",
    "            print('Epoch ',epoch+1,' completed out of ',hm_epochs,', loss ',epoch_loss)\n",
    "        \n",
    "        \n",
    "        correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "        accuracy  = tf.reduce_mean(tf.cast(correct,'float'))\n",
    "        print('Accuracy: ',accuracy.eval({ x: test_x, y: test_y }))\n",
    "\n",
    "        \n",
    "# hm_epochs = 4, accuracy = 93%\n",
    "# hm_epochs = 10, accuracy = 95.14%\n",
    "#hm_epochs = 15, accuracy = 95.54%\n",
    "train_neural_network(x)        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n",
      "423\n",
      "Epoch 1 completed out of  10  loss:  236152.9129638672\n",
      "Epoch 2 completed out of  10  loss:  109876.04608154297\n",
      "Epoch 3 completed out of  10  loss:  66765.44448852539\n",
      "Epoch 4 completed out of  10  loss:  42331.59622192383\n",
      "Epoch 5 completed out of  10  loss:  27228.843627929688\n",
      "Epoch 6 completed out of  10  loss:  19795.766136169434\n",
      "Epoch 7 completed out of  10  loss:  20923.42981338501\n",
      "Epoch 8 completed out of  10  loss:  16688.859075546265\n",
      "Epoch 9 completed out of  10  loss:  12001.857012748718\n",
      "Epoch 10 completed out of  10  loss:  21833.35897731781\n",
      "Accuracy:  0.5778612\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "hm_lines = 100000\n",
    "\n",
    "def create_lexicon(pos,neg):\n",
    "    lexicon = []\n",
    "    with open(pos,'r') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents[:hm_lines]:\n",
    "            all_words = word_tokenize(l)\n",
    "            lexicon += list(all_words)\n",
    "\n",
    "    with open(neg,'r') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents[:hm_lines]:\n",
    "            all_words = word_tokenize(l)\n",
    "            lexicon += list(all_words)\n",
    "\n",
    "    lexicon = [lemmatizer.lemmatize(i) for i in lexicon]\n",
    "    w_counts = Counter(lexicon)\n",
    "    l2 = []\n",
    "    for w in w_counts:\n",
    "        #print(w_counts[w])\n",
    "        if 1000 > w_counts[w] > 50:\n",
    "            l2.append(w)\n",
    "    print(len(l2))\n",
    "    return l2\n",
    "\n",
    "\n",
    "def sample_handling(sample,lexicon,classification):\n",
    "    featureset = []\n",
    "    with open(sample,'r') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents[:hm_lines]:\n",
    "            current_words = word_tokenize(l.lower())\n",
    "            current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
    "            features = np.zeros(len(lexicon))\n",
    "            for word in current_words:\n",
    "                if word.lower() in lexicon:\n",
    "                    index_value = lexicon.index(word.lower())\n",
    "                    features[index_value] += 1\n",
    "            features = list(features)\n",
    "            featureset.append([features,classification])\n",
    "    return featureset\n",
    "\n",
    "def create_feature_sets_and_labels(pos,neg,test_size = 0.1):\n",
    "    lexicon = create_lexicon(pos,neg)\n",
    "    features = []\n",
    "    features += sample_handling('pos.txt',lexicon,[1,0])\n",
    "    features += sample_handling('neg.txt',lexicon,[0,1])\n",
    "    random.shuffle(features)\n",
    "    features = np.array(features)\n",
    "\n",
    "    testing_size = int(test_size*len(features))\n",
    "\n",
    "    train_x = list(features[:,0][:-testing_size])\n",
    "    train_y = list(features[:,1][:-testing_size])\n",
    "    test_x = list(features[:,0][-testing_size:])\n",
    "    test_y = list(features[:,1][-testing_size:])\n",
    "\n",
    "    return train_x,train_y,test_x,test_y\n",
    "\n",
    "train_x,train_y,test_x,test_y = create_feature_sets_and_labels('pos.txt','neg.txt')\n",
    "\n",
    "\n",
    "n_classes = 2\n",
    "batch_size = 100\n",
    "\n",
    "x = tf.placeholder('float',[None,len(train_x[0])])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "train_x,train_y,test_x,test_y = create_feature_sets_and_labels('pos.txt','neg.txt')\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "x = tf.placeholder('float',[None,len(train_x[0])])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def neural_network_model(data):\n",
    "    hidden_1_layer = {'weights': tf.Variable(tf.random_normal([len(train_x[0]),n_nodes_hl1])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl1,n_nodes_hl2])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_hl3])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl3,n_classes])),\n",
    "                      'biases': tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    l1= tf.add(tf.matmul(data, hidden_1_layer['weights']) , hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2= tf.add(tf.matmul(l1, hidden_2_layer['weights']) , hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3= tf.add(tf.matmul(l2, hidden_3_layer['weights']) , hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3, output_layer['weights']) + output_layer['biases']\n",
    "    return output\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    hm_epochs = 10\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss=0\n",
    "            i=0\n",
    "            while i < len(train_x):\n",
    "                start = i\n",
    "                end = i + batch_size\n",
    "                batch_x = np.array(train_x[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "\n",
    "                _,c = sess.run([optimizer,cost] , feed_dict = {x: batch_x , y : batch_y})\n",
    "                epoch_loss+= c\n",
    "                i+= batch_size\n",
    "            print(\"Epoch\",epoch+1 , 'completed out of ' ,hm_epochs, ' loss: ', epoch_loss )\n",
    "\n",
    "\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy: ', accuracy.eval({x:test_x , y: test_y}))\n",
    "        \n",
    "\n",
    "        \n",
    "train_neural_network(x)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1.],\n",
       "        [1., 1., 1.]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 2, 34, 5, 6, 67, 6], [1, 0]]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "le = [1,2,34,5,6,67,6]\n",
    "ls = [1,0]\n",
    "l.append([le,ls])\n",
    "l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
